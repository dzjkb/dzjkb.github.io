damn this https://www.depthfirstlearning.com/2021/VI-with-NFs stuff seems really worthwile, but de facto a whole course on the topic
## experiments
still overfitting??????????
- how tf my man
also, the discriminator warmup doesn't seem to happen?
- wait it's only 5k steps ughhhhhhhhhh
- did happen, way too little though

so, is something maybe wrong with the KLD regularziation?
or is the posterior too expressive still?
- less parameters? more regularization weight?
how do I verify this?
browse through NF posterior implementations
- ok so why am I not doing just `_latent_loss()` on `z_flow`?
- that's what the [flow-VAE](https://github.com/fmu2/flow-VAE/blob/main/static_flow_conv_vae.py) impl (see [[normalizing flows]]) does
- so the flow input distribution is actually regularized + log_det term regularizes the flow itself
- as opposed to the current situation where we regularize the input distribution to follow itself(?) plus the approx. posterior to follow an isotropic gauss, which is what the `normalizing-flows` package [example](https://github.com/VincentStimper/normalizing-flows/blob/master/examples/vae.py) does
- other random impl does the same thing as above

the loss curves are very similar to the last run w/ 256 latent size, except latent loss being lower (due to higher weight)
## sem prez
3 days oops
continue ideas from [[2025-12-30]]