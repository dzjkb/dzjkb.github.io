## experiments
not now, but they're listed in [[2025-12-16]]
## normalizing flows
oh boy - [[normalizing flows]]

Qs
- does using flows for the prior makes sense too? any examples?

ok shit
looking at the original [[VAE]] paper, the loss is computed analytically specifically for the multivariate gaussian case
when both the approximate posterior and the prior are such distributions
would KL divergence for flow-transformed distributions even make sense?
would it have to be estimated by sampling?
from [Variational Inference with Normalizing Flows](https://arxiv.org/pdf/1505.05770):
```
Monte Carlo approximations
(including the KL term in the bound, if it is not analytically
known).
```
yes obv it would make sense, KL divergence by definition doesn't assume distribution form, you just need to evaluate the probs at each point bruv
but the formulation here is totally wild

the VAE example from `normalizing-flows` has the following KLD term (`z_` is after flows, `z_0` before, `log_det` is the flow volume term probs):
```
kld = (
    -torch.sum(p.log_prob(z_), -1)
    + torch.sum(q0.log_prob(z_0), -1)
    - log_det.view(-1)
)
```
where `p` is a `Normal(0, 1)` distribution?
wait we do compare the approximate posterior with the prior in the original [[VAE]] though, right, that's the point
KL divergence just being the sum of log probs, why tho, doesn't fit the paper thingy at all
or does it? there's the `q0`