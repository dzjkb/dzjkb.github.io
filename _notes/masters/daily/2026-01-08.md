    ahhhhhhhhh
## experiments
shit
what do
- verify latent loss is correct
- hyperparameter changes
    - less parameters
    - even larger latent loss weight
- investigate whether this is not posterior collapse
    - try without NFs?

I should just get going with datasets and metrics
meanwhile, running with no NFs
- ok ok 15 epochs in the train loss seems to be properly decreasing
- 90 epochs in the reconstruction loss curve is pretty much identical to the NF version hmmmmm oops
    - might make sense if it's posterior collapse => the decoder is doing all the work?
- ok 550 epochs in, it's the same shit
    - and the latent loss stabilized, meaning the posterior should look alright
    - try smaller model
1 less layer gives 10x parameter reduction, oh boy let's see
- ok ok it's learning something
- idk might just flatten out at a higher value? train loss seems to be doing just that at ~5.2
- maybe it's the reduction to a single latent - try `.mean()` next??
## misc
oh shi
- [ ] read [AI (r)evolution -- where are we heading? Thoughts about the future of music and sound technologies in the era of deep learning](https://arxiv.org/abs/2310.18320)

```
As far as the grander narrative of computer music research is concerned, this is a distinct shift from the practices of our previous cultures of research - those whom, like Jean Claude-Risset and Miller Puckette, engaged in their research as means of furthering both their own creative practice and the landscape of potential creative techniques and idea
```
: ooo
![[Pasted image 20260109004235.png]]
amazing

#### [Applications of Artificial Intelligence in Music: A Review](https://www.sciopen.com/article/10.26599/IJCS.2025.9100011)
#### [Artificial intelligence in music: recent trends and challenges](https://link.springer.com/article/10.1007/s00521-024-10555-x)
## sem prez
tmrw aaaaaa
continue ideas from [[2025-12-30]]

best idea would be to speak about my specific mgr stuff to move the whole topic forward, rather than various fun curiosities

diversity stuff?
- [[Frechet Audio Distance]], [[Kernel Inception Distance]]
    - and explain CLAP briefly
- [[DrumGAN]]
- [[Crash]]
- other papers doing this sort of generation?
and [[normalizing flows]]
- idea, theory
- using in VAEs
- any usage in music?
- idk but [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2403.03206) is a quite fresh paper, 3k citations, using flows

end of prez summary:
- model - VAE with residual+convolution blocks, single latent vector, NFs for prior/posterior
- datasets - samples, fx, cut tracks, ?
- evaluation - embedding-based metrics
- experiment plan:
    - with/without NFs
    - compare NFs with a trained [[RAVE]] prior

sidenote: what happened in the space after [[RAVE]]?
- [[that guys phd]] thesis lol
    - latest work is [here](https://arxiv.org/abs/2408.00196)
    - but he's also a co-author on this - [[Platune]]
- disentaglement, knowledge distillation thingy
- whatever magenta is up to?

- tools creating music - suno and the likes
- soundtrack generation ([[MusicLM]]) for ads, games etc.
    - tf was that zaiks thing called?
- speech synthesis
