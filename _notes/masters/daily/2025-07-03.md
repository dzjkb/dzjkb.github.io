### nans
the losses are stable now
still NaNs out at 100 epochs
or not, this time, bro what
- a watched pot never boils
- latent loss went crazy (3e11) at like 180 epochs though, what
- still going strong at 230, it's like it's being annoying on purpose
- 100 epochs for the LR decay, bit too little actually? considering we're going down * 0.01
- cut it short, f it
### datasets
- [x] prep a command that splits an audio file into a set of overlapping clips of a given length, resampling if necessary

need to do some soulseek diving for chopin
also, try Nils Frahm - totally different characteristics, should be simpler to learn?
- [x] Nils Frahm - Graz dataset
- [x] the graz clips have 2 seconds instead of 4 for some reason, why?

Also, time for Irisarri or Hecker
- [x] Irisarri dataset
### results ?
oh shit this is actually starting to sound good
filtered noise seems to be working, giving a normal snare sound instead of a bitcrushed one

I should try non-variational autoencoders
- [x] make being variational optional

graz - audible piano, but covered in a dense layer of static
training failed after 238 epochs
extreme spikes in losses, maybe jumped too high at some point?
what could be the cause of this? how to diagnose/fix?
could try gradient clipping, but that could be symptomatic

ooooohhh shit
what if the model size isn't enough for the whole-band signal
16x greater resolution than the signal RAVE convs are dealing with
wait shit I'm doing way less capacity anyway, 16 instead of 96
try 8 times bigger (128) (600mb model oh boy)
- [x] what hidden size did [[SoundStream]] use?
### next?
goals, evaluations, thesis planning
datasets
hyperparameter tuning, planning out experiments

see how far I can get without the additional discriminator training

training 128 capacity, now this is going to take a _while_
and it's NaNing on the 0th epoch are you kidding me? - L2 loss again - 1e-4 initial lr helps, runninxg
ohhhh it's converging nicely!?
loss similar to the previous ~200 epoch losses at epoch 30
remove the pad/cut augmentation (1.5s) though...