## experiments
first training run that finished in a while lmao
- segfault at the very end after the profiler report, hm
- the MultiScaleSTFTLoss is on cpu? just the launching functions probs, but it seems like a very natural candidate for graph compilation
losses
- discriminator loss is shaky, but decreasing nicely
- adversarial loss is growing
- feature matching loss is growing
- (mb) reconstruction loss is decreasing nicely with an expected curve, sort of like what would happen if it were just encoder training?
- validation reconstruction loss is still the same old shenanigans
- the static on validation samples is similar to the only-encoder runs
- it was a bigger noise generator, hard to tell what the influence of that was
conclusions:
- feature matching/adversarial loss weights are too small
- discriminator lr seems ok

next?
- increase feature matching/adversarial loss
- increase number of epochs - 3k
- increase encoder training - 150k steps
ok it's plateauing at ~13 again fmlllllll
- after around 15 minutes
- try running a few times, see if it gives
- yeah third run w/ the same config is behaving ~~properly~~ plateauing at 11 this time brooooooooo
- mmkay the lr isn't stepping at all fucking hell man
- let's try again bih
- ok now it's working properly, right ok dude fccc
ok sounds pretty much the same as last encoder training with these parameters except the noise gen
- guess the noise generator doesn't make that much of a difference

and what after?
- increasing capacity? latent size?
- try running this on soft channel
- rave reproductions
## datasets
buy an external HDD - download [[LAION-audio-630K]]
browse, filter, process

in the meantime, add the downloaded things to `snares2`
- in `uni/mgr/sample_packs` or `/media/Data/jp_dir`
    - FSDKaggle2018, random reddit stuff
 
process [FSD50K](https://zenodo.org/records/4060432)