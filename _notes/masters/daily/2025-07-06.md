---
tags:
  - aaa
---
### nans continued
NaN at epoch 135, fuck, it's not just capacity
I assume it should be able to learn higher quality reconstructions but something is in the way
**important:** the latent loss explodes periodically to extreme values, coming back down to normal levels after a few epochs
- actually exploded once this training, NaN appeared before it fully came down
- step ~52k, while beta warmup lasted 20k steps
ideas? (shoutout Claude)
- STFT returning extreme values - not the case now, might've been the case in other trainings where the reconstruction loss exploded
- LR - either the graph is screwed up or the schedule is screwed up
    - wait CosineAnnealing is cyclical? fuck
    - just do `LinearLR` without overthinking this
- **VAE posterior collapse** - posterior becoming identical to the prior - references?
    - [[VAE posterior collapse]]
    - maybe just decreasing beta would work? haven't touched that yet
    - collapse - posterior ~= prior
    - wouldn't that mean notes (rythm + pitch) wouldn't be exactly reproduced in the graz dataset? maybe this isn't the problem afterall?
    - beta warmup is supposed to alleviate this, maybe tuning the hyperparams (longer warmup) should be enough?
    - ! different [annealing regimes for beta](https://www.microsoft.com/en-us/research/blog/less-pain-more-gain-a-simple-method-for-vae-training-with-less-of-that-kl-vanishing-agony/)
        - beta restarts?

---

changing example length to 3s gives 8G required per one sample -> `batch_size: 1`
15min per epoch!? => 25h for 100 epochs
`capacity: 64` and `batch_size: 2` => ~4.5min per epoch, 7.5h for 100 epochs
ok
### logging
some way of monitoring the latent space would be nice
not sure the 3d projection is that useful?
what would I need to monitor though? mean/std of each dimension?
### misc
post idea - learning VAEs in 2025
- [x] weight norm

any difference in returning `logvar` vs. `std` from the encoder?

VAE example impl https://chrisorm.github.io/VAE-pyt.html