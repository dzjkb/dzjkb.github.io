## experiments
ok this shit is getting ridiculous
still overfitting, although the validation loss is shifted downwards by a constant

discriminator warmup still doesn't seem to be working
okkkkkkk no it does work, it's just the total loss values that change because of additional summands

how tf is this still overfitting with such a small model size + only 128 latent size
- maybe the train/val data are _too_ different for this kind of task?
- a uniform random split over the whole album would be too much like test set leakage though
- maybe try less capacity but a deeper model (=> smaller linear layers at the end but rather fully convolutional downsampling)

ok wtf the super small capacity still fits the data quite well (3.5 reconstruction loss after 5.5h) but the validation loss is going cray
- the weird thing is 3.5 train loss should give at least good sound quality, but the val examples have tons of static
- so did the previous val examples though hmmm
so tf is happening
- the bottleneck is too tight?
    - what if NFs would help with that?

idk just try snares
730 training samples not much eh
- [ ] add padding to noise/encoder/decoder to not need to adjust strides to training data length
## next
ok ok so
really need to get going with datasets
- the LAION things
- also whatever [[DrumGAN]] was trained on?
then, launching baselines on the datasets
- compare to [[RAVE]] and [[Crash]]? (or [[DrumGAN]]) not sure crash is open, def wouldn't reproduce it on my own
- need to figure out how to do 1s samples priors with RAVE
and then => formalizing the contributions of my sampler