## experiments
ok ok ok the validation loss actually started to decrease normally?
but
- the validation samples are screwed up from the start - somehow mashed up, both the original and reconstructed validation audio
    - this might be why the validation loss is super high?
    - ok it's just samples that are loops...
    - [x] remove loop samples from snares1
- every loss just fkn explodes when discrimination comes in bruh
    - maybe it's the massive feature matching loss weight?
also, try tanh for the single latent reduction instead of ReLU since that cuts off half of the available latent space
- or maybe not since that's _before_ the last linear layer, but still, it's only linear? idk

ok ok done some fixes
- leaky relu .2 slope instead of .01
- tanh instead of relu for single latent reduction
- 4, 4, 4, 2 strides
- removed loops from train/val sets
running
ok wait what the latent loss went to literally 0, while reconstruction loss is plateauing at 12-13
decrease latent loss /5, run again
#### next?
- FAD/KID for a given model using CLAP
- CLAP embedding visualization for a given (model? set? a couple of sets? or both at the same time really)
extract a model unconditionl gen + dataset embedder + FAD/KID on embedding set
## datasets
freesound what do

get going with browsing sample packs...
## optimization
I should probably spend like a day or sth on profiling and trying to speed this up
## misc
latent space paper [[RAVE]] refers to https://arxiv.org/abs/1706.00409v2
- read?