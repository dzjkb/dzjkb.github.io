## experiments
fuck, still overfitting, even though the train loss is at a nice 3.1 and the latent loss is low
- training curves are similar to previous runs
what do?
- even less latent dimensions?
- or smaller model?
- or more latent regularization?

ok increase latent weight slightly + decrease latent size to 128