model, training
(also read przemeks script brooo)

https://caillonantoine.github.io/ :eyes:
https://www.youtube.com/watch?v=y7gKlzvg8xk ohwow

cv tool ??
https://github.com/elipapa/markdown-cv

---

summary of what's happening in the RAVE code in [[RAVE]]

some additional repo with transforms?
https://github.com/caillonantoine/UDLS/blob/master/udls/transforms.py
available on pypi

also this seems useful - gpu utils
https://pypi.org/project/GPUtil/

libs potentially worth looking into?
- gin for configuration - not sure really
- absl for cli apps - nah, click if anything
- pytorch lightning - yes, probably a good idea ([intro - docs](https://lightning.ai/docs/pytorch/stable/starter/introduction.html))
- GPUtil - yes
- einops - maybe? reading more RAVE code might show

---

training script seems simple enough with lightning, mainly data loading

the model though? `model::RAVE` is the lightning module, with actual nets defined in (? `blocks`) - passed in as arguments to `__init__()`, I hate this
ok admittedly the gin configs are nice later, different versions with different decoders etc

this isn't thaat much code really

no ddsp component in the generator actually? wtf
the `NoiseGeneratorV2` module seems to just be
- a series of convolutions that produces amps
- that get converted to IRs
- that get convolved with a random (noise) signal
it's like `ddsp_pytorch` is a submodule but it isn't used??

tough, guessI really do need to understand this though
but maybe just don't use cached convolutions? sounds like unnecessary additional problems

---

ok getting a model going seems doable
way more stripped down than what's here, at least for starts
- no cached convolutions
- no pqmf
- ?