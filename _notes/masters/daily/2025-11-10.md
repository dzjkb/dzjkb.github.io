## goals
pretty clear, tasks are listed in [[what do (masters)]] - on hold right now
next step would be metrics
## experiments
!?
omg it's still going
losses jumped considerably for phase 2, still haven't fell to the phase 1 lowest level
including reconstruction loss, huh (shit)

but it does sound better regardless, huh?
just like if it had more reverb and was a bit more hazy
but less of the static noise
interesting, hope that doesn't mean the multi-scale STFT loss isn't that "perceptual"...
- the higher reconstruction loss sample from epoch 500 does sound better than the epoch 220 one though, shit
- I mean perceptual loss is one thing, then there's also the adversarial and feature matching, maybe that's a more complete image of the "perception" of sound
shit am I hearing phase artifacts actually? the original doesn't sound that good either

looking at [[RAVE]] training times this should be a lot longer
but this already takes ages
I need faster gpus???
try tuning stuff like lr+betas first, also see if snake+augs help
## evaluations
[[diversity metrics]] - discrepancy
what do the other papers write about diversity?

[[MusicLM]]
- multiple generating runs from the same prompt
- in our case the decoder is deterministic, so that doesn't make much sense

[[RAVE]]
- nothing in the article really

next - TBD
## next
k I mean what are the concrete things I can do - implement
- rest of the datasets
- normalizing flows
- reproducing previous results
- and f-ing unconditional generation if I want to do any evaluations on that bruv

setting up AWS to do the work?
## datasets
ok soft channel + debussy preludes + chopin scherzos
how tf do these datasets relate to the goal of generating samples
I should probably create more sample datasets (other instruments, SFX banks?)
- soft channel is kind of a SFX dataset lol
## misc
https://acids.ircam.fr/course/makimono/ :o