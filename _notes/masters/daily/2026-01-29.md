~ tf is this
~ https://kyutai.org/blog/2025-07-03-kyutai-tts-1-6b
~ ohh shit Defossez and Rouard hah
~
~ tf
~ [Continuous Audio Language Models](https://arxiv.org/abs/2509.06926)
~ this is like the RAVE latents prior on steroids
~ https://huggingface.co/spaces/kyutai/calm-samples love it
## experiments
ok shuffling `soft_channel` improves the situation slightly (more harmonic content), but it's still just erratically fluctuating static
- how is the reconstruction loss so low though
- shows what's prioritized by the STFT loss?
loss investigation:
- [[RAVE]] does magnitude manually on the complex spectra, while I pass in `power=1` - shouldn't make any difference I hope
- _not_ using the melscale should prioritize higher frequency content, which is what we want for accurate timbre reconstruction rather than just volume envelope following

ok why am I doing like the hardest difficulty level, try peripeteia or debussy?
running peripeteia

ok damn it's just dense static
only 2h admittedly ok ok what am I expecting
it's like the loss values don't scale between datasets

ok I really need to do the baselines first
## reading
look at [[CALM]] evaluation methods (they use [[Frechet Audio Distance]] - how?)
- no details in the paper
## dataset
#### percussive1
next step - unzip and move around the things in `sample_packs/`
FSD50K - not sure it's what I need, "audio events"

ok ok wait we have like 7k samples now? that's quite a bit
run a well configured training on this during the leave, see what happens
- verify there's no ~10 plateau fuckk
- verify discrimination stage won't OOM
90/10 split cuz
#### further stuff
something with sfx? cmon

simple albums - still not sure the random cutup structure is something I should expect from the model, and it's defo not something I would be reporting in the paper anyway

what would I be reporting?
- snares, other percussive sounds
- voice? like single words or utterances?
    - getting into voice datasets (VCTK) would open a whole new world of comparison
    - what if it would learn an encoding for words, with similar phonetics -> close embeddings? oh shi
    - I could think about automatically cutting up recordings into words based on time-aligned transcriptions and/or silences
- SFX or other datasets I would emphasize are _novel_
#### voice
VCTK doesn't have exact word timestamps
Libri-TTS has sentence breaks marked, still not enough
there's this - [Castella](https://github.com/line/CASTELLA) with sub-sentences timestamped
and then there's this - [AudioTime](https://arxiv.org/pdf/2407.02857v1) which looks like what I need?
- [gh pages](https://zeyuxie29.github.io/AudioTime/)
still, comparing myself on VCTK would be nice
## next
bro
- expand `percussive1`
- reproduce [[RAVE]]
- prepare the voice dataset
- sfx dataset