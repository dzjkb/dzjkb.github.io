## experiments
ok ok unconditional generation will be getting saved in an eval dir
try getting a reasonable discriminator run -> run FAD/KID on AWS
- starting from snares v14

running the optimized discriminator
8x slowdown for discriminator warmup
ok ok speeding up lesgo, 3 quick 1 slower step
bruvvvvvvvvvvvvvvvvv
it's finally beginning to behave reasonably
reconstruction loss is actually falling further, feature matching/adversarial loss is oscillating with a decreasing trend
lesssgooooooooooooooooo
hm first validation epoch in discriminator phase isn't showing much of an improvement, but we'll see
## dataset
what about the [10k percussive one-shots?](https://zenodo.org/records/3665275)
- nope, they're 16kHz

ok ok ok Golui got some ideas
buy an external HDD - download [[LAION-audio-630K]]
browse, filter, process

in the meantime, add the downloaded things to `snares2`
- in `uni/mgr/sample_packs` or `/media/Data/jp_dir`
    - FSDKaggle2018, random reddit stuff
 
process [FSD50K](https://zenodo.org/records/4060432)
## profiling
ok now's the time to speed this shit up
viewing pytorch traces (`.trace.json` files): https://ui.perfetto.dev/
fuck idk what to read from this, but it's a nice profile of how much time each stage of a step takes

`@torch.compile`???
fuck requires torch>=2.0

ok so the autograd after discrimination takes a looooong time
and so does the adam step
and like over half of it is fucking `zeros_like`?????
rightttt it might be the `zero_grad()`?????
zeroing gradients accumulated throughout 4 steps?
removed discriminator loss calculation in non-discriminator steps
fused Adam - torch>=2.0
- [ ] add `torch.compile` and `Adam(fused=True)` once using torch>=2.0
~~ok maybe it's a bit faster? idk, minimal difference~~
traces still look the same
maybe it's a first-time allocation??
- don't think so, two subsequent epochs took ~3min to complete
it's really the `zero_grad()` cmon what

the MRD discriminators are taking long too
mostly convolutions what are you going to do

ok Claude you cunt, you actually did this
```
## The Problem in Detail

When you call:

_, _, gen_loss, losses_dict = self.forward(batch, batch_idx=batch_idx)

Inside `forward()`, you call `self._discrimination(x, x_hat)`, which runs:

all_discriminators_real_feature_maps = self.discriminator(x)
all_discriminators_fake_feature_maps = self.discriminator(x_hat)

This means `x_hat` (which depends on encoder/decoder) and the discriminator outputs are both part of the computation graph. When you later do `.backward()`, **both** the generator and discriminator parameters accumulate gradients.
```
solution: run encoder/decoder with `no_grad():` if I'm in the discrimination step
- or do `gen_opt.zero_grad()` after the discrimination step? but `no_grad()` seems more robust
okkkkk that's what I'm talking about
visible hiccup every 4th step, but still way faster than previously, 1min epoch with the largest discriminator
fuckkkkkkkkkkkkkkkkkkkkkkk

next? the torch 2.0 things, losses and discriminators take up most of the time and I bet they're easy to compile and speed up
## misc
reading in [[2026-01-21]]

oh my fucking god
the dataset class is caching examples _after_ augmentations............
I swear to god