this is going to be a single note for the whole France trip (2026-01-31 - 2026-02-08)

new Mistral transcription model btw, Voxtral 3
![[Pasted image 20260205181201.png]]
:>

maybe I should send another email to Simon to his newer address?

https://www.zaiks.org.pl/dla-autorow/fundusz-popierania-tworczosci
## experiments
oh my god this run is ffuuuucked
after 100k steps of plateauing at 10 the loss exploded back to 12-13
just staying there
what the actual fuuuuuuuuck
and the latent loss also started increasing, interestingly enough
no dimensions are under the free bits limit
ridiculous

try that fkin 64 capacity idk man
plateauing at around 9
ok maybe it's just this dataset that's shit
too, I don't know, varied? loops? sth id
plateauing at around 8.5, eh
maybe it's the gradient clipping?

no gradient clipping, looks less like a plateau but it still fell only to 8.8 at 400k steps with some very suspicious spikes
the gradient norm shoots up to ~5e4 at times
bullshit

something has to be wrong with the data
- loops? I really hope there aren't some accidental loops in there
- too hard of a problem? no way
## dataset
[[VCTK]] words let's go