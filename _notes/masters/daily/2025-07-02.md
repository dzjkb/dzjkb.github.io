~ o    ~
~ oh shit what is this https://github.com/audiophil-dev (gencaster repo)
### nans
the rerun went way longer without failing for `Conv1d` for some reason, but threw a `NaN` for the L2 loss again, what
the fuck

something to probably answer in the future:
- [ ] why lin + log distance instead of one or the other?

shit maybe it's actually exploding gradients/loss?
the latent loss went cray, reconstruction loss started to rise heavily
- Adam parameters?
    - RAVE uses `(0.5, 0.9)` instead of the default `(0.9, 0.999)` which is quite a big difference
- gradient clipping? might be a "symptom fix"
- a LR schedule - just a lower LR might do the trick actually
I should try a random search over these parameters
- not sure the loss would NaN in this case though - rather inf?
- being NaN would still require the values to be 0, which they can't be due to eps
- whyyy

added gradient norm monitoring, slows down the training like 3-4 times oops
I should probably start the training from the beginning, since the LR schedule might not work properly if it's not present in the checkpoint

the noise module miiight fix this somehow? if the model is actually able to learn the timbre more easily

nans in epoch 72 i fucking swear what is this
damn the kicks from epoch 67 sound good though
as if the lr changes made it converge waay faster? the loss is also very smooth and reasonable now
it's the `DivBackward0` again though come on man
and losses aren't exploding at all this time, `grad_norm` has totally normal values
uhhhhhhhhhhhhhhhh

what about symmetrical padding though? maybe that is the answer after all
the run breaks on epoch 78 on the same thing
- [x] try symmetrical padding (to rule out it's not the cause of NaNs)
### noise
shszshsh
```
why doesn't Antoine's noise module have the same windowed application (=> time-varying) of the filter as the original ddsp repo does?
Antoine why u do this to me
```
just fucking do these ops in a notebook???

ok they seem to run correctly, cant tell if the sound is good since random magnitudes just give random noise anyway, but it seems a bit low-pass filtered, so that's a good sign

try adding to the model and see what happens

done
it's moving??? holy shit
RAVE adds `torch.tanh()` at the end of the decoder, their ddsp doesn't, hm
- seems like the model started to stay in the [-1, 1] range by itself now???
- niceeeeeeeee
log noise and waveform signals separately? might be a bit too hacky