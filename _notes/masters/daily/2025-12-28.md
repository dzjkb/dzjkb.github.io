ok ok ok ok
## normalizing flows
guess I could just try a test run?
for a couple of days on `gpu2` why th not
## experiments
uhh without NFs maybe first? to have a proper baseline
but with proper discriminator pre-training
new things to try:
- disciminator pre-training
- single latent vector

config?
- the latest runs were `graz_no_adv_3[_mono].yaml`
    - do mono first so there are less potential complications
- do `discriminator_warmup_len`
    - 5k steps would be ~50 epochs, sounds good
- do `fixed_length` 
    - increase latent size to make up for the 300x smaller latent space
        - jeez that's quite a bit, 100Hz was a lot for the latent space
    - so like `latent_size: 2048`? 16x increase from 128, still not much
    - the quality is just going to be shit
    - I should try a different dataset for this, go back to snares?
    - maybe I should
- do NFs since I just wanna get something working ASAP and go from there
    - ablation studies after I get shit working nicely fr
    - 16 NF layers idk
- 97 steps per epoch

a more systematic approach would be niceeeeee
maybe I don't need these experiments right now
ooor maybe try 1s graz, almost like a sample
100 -> 16 would be a 5-6x latent size decrease, ok maybe maybe
considering the NF addition

see what model sizes are with/without NFs + latent size increase + single latent
- `from pytorch_lightning.utilities.model_summary import summarize`

`graz_no_adv_3_mono.yaml`: (logs in `graz/version_2`)
```
  | Name                | Type               | Params | Mode  | FLOPs
---------------------------------------------------------------------------
0 | encoder             | Encoder            | 5.3 M  | train | 0    
1 | decoder             | Decoder            | 6.0 M  | train | 0    
2 | pqmf                | PQMF               | 0      | train | 0    
3 | discriminator       | Discriminator      | 33.8 M | train | 0    
4 | reconstruction_loss | MultiScaleSTFTLoss | 0      | train | 0    
---------------------------------------------------------------------------
45.2 M    Trainable params
0         Non-trainable params
45.2 M    Total params
180.637   Total estimated model params size (MB)
390       Modules in train mode
0         Modules in eval mode
0         Total Flops
```

`graz_full_mono_v0.yaml`:
```
  | Name                | Type               | Params
-----------------------------------------------------------
0 | encoder             | Encoder            | 12.7 M
1 | posterior           | RealNVP            | 2.2 M 
2 | prior               | RealNVP            | 2.2 M 
3 | decoder             | Decoder            | 9.7 M 
4 | pqmf                | PQMF               | 0     
5 | discriminator       | Discriminator      | 33.8 M
6 | reconstruction_loss | MultiScaleSTFTLoss | 0     
-----------------------------------------------------------
60.6 M    Trainable params
0         Non-trainable params
60.6 M    Total params
242.211   Total estimated model params size (MB)
```

mmkay it's the latent size that increases the size so much

right, shapes
why tf is `ld_` of the same shape as `z` - `(batch_size, latent_size)`?
and why tf does the flow return `x: torch.Size([4, 2048, 2048])`????
nooooooooo