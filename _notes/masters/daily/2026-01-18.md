## experiments
ok ok so getting rid of the noise generator reacts worse with the discriminator phase
- or maybe it's the higher discriminator learning rate
higher latent loss, higher validation losses

idea: put the noise gen under amp mod?
- why isn't it there in the first place?
- ~~maybe because it's being run on the already amp modded signal?~~ it's run on the raw net output
#### next
- why plateauing?
- reproduce [[RAVE]] on snares?
- model optimizations
- compare kid with/without NFs
running w/ larger noise module
- ok wtf doing 4, 4, 4 strides makes it totally break
- but then going back to 2, 2, 2 but 5 filters instead of 6 breaks it too???
- wtf
- ok I've recreated the version_11 config and it's still plateauing at 13-14, something's screwed up big time
- whattttttttt
- the latent loss goes up way higher at the start before
- oh my fucking god, it's the discriminator lr
- setting it back to 1e-4 fixes it you gotta be fucking kidding me
- no but version_12 had 5e-4 too and it learned the encoder well
- ok no suddenly everything is ok again, what
ok running with larger noise module again, this time it's super similar, minimally lower losses but the sound is pretty much the same
## eval
ok run embedder + KID, compare with idk KID of the train+test sets
add KID to validation epoch
- when do I embed the validation set? need a standard location for that, like `<dir>_embeddings`
- the model doesn't have the dataset paths though, additional argument? yeah `kid_reference_set` makes sense
impl FAD, add to validation epoch
#### FAD
some impl thoughts in [[2025-12-10]]
yeah maybe it's easier to just use the fadtk cli
works with `CUDA_VISIBLE_DEVICES=""`, nice

ok uhhhhhhh done?
launch, test
fadtk computation might need to be moved to gpu
## datasets
freesound uhhhhh

write to the [[Crash]] authors????