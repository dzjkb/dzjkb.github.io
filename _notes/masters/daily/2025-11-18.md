ok tailscale done nice
don't want to add `gpu2` to my network though, so I still need to fill in the address at every re-launch
## unconditional generation
bruh
saving audio?
## reproducing RAVE
on graz, soft channel and snares
see what specs did [[RAVE]] use (see todos)
## experiments
running `graz_mini` continued from 600epoch checkpoint
OOM after 25 epochs wtf
fails immediately after launching it again? how tf
I can't just change the model config now to use less memory
rightttttttt this is when the discriminator kicks in.............
fuck
what if I do just change the batch size
sue me
`batch_size: 3` uses almost all of the 15G now, shit
almost ~~3 minute epoch, shiiiiiit~~
ok 1:40 not that bad but still 1.5x slowdown
## other
use [torch compile](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)???
- compiling `hinge_gan_losses` makes it run slower lol, maybe the overhead from loading the model is too big

`cuda.empty_cache()` doesn't make sense if I'm running into OOM right at the beginning, before `backward()` even

other stuff to try
https://medium.techkoalainsights.com/7-hidden-pytorch-memory-optimization-techniques-that-cut-gpu-usage-by-50-01920bf88951?gi=679c041b0321