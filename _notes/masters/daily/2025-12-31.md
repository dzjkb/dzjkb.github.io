## experiments
fuck the overfitting is real, even with bigger latent loss weight
why?
- posterior is _too_ expressive?
- or maybe the latent size is just too big?
    - there are 1930 training samples and 2048 dimensions, right
    - that would make a lot of sense, try 256?
running again with latent size 256 and batch size 8

nans goddammit
maybe it's the FC's accumulating gradients?
I could do a mean over the conv outputs instead of the FC
- configurable => experiments?

ok changed model size, no more nans, let's see
## sem prez
?