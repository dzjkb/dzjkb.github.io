## experiments
it's still overfitting whattttttttttt
crazy, something's off here

monitor latent space somehow?
- that's ready to go actually
do `.mean()` instead of a linear layer to reduce to 1 latent
- I can't do the inverse with a `.mean()` though, has to be a linear layer I guess
- or some sort of weird transposed convolution?
- any references? what do other works do?
ok wtf, changing the encoder linear layer to just mean reduction doesn't change the total numer of parameters
was it actually just that few parameters? reducing (128, 100 -> (128, 1) is like wait wait wait
I should be reducing over the channel dimension not the length dimension
fml
ok reshaping n shi
this is weird the decoder grew over 2x more than the decoder
## sem prez
aaaaaaaaaaaaaaaaaa
ok ok just info brain etc. dump

podkreślić contribution
w jaki sposób zmiana arch wpłynie na zmiane wyniku?
iteracyjne generowanie Zów => sampli?