## experiments
same amount of epochs but the quality is considerably worse
maybe higher lr?
maybe I should reduce the discriminator size? 2x bigger than the encoder/decoder

oh wow ok the scheduler isn't working if I'm not calling it oops

ok `soft_channel_mini` might be too hard to model, curves are way higher than `graz_mini` at corresponding timesteps
also just the difference in sound between different tracks might be too much to reasonably perform on the validation tracks (set)
maybe I don't need to do it on `soft_channel`, just choose one dataset and stick to it
graz is too simple and similar to the original datasets
but stick to graz for now since it's been giving some results?

check [[RAVE]] parameters etc
- 3M steps (1.5M for phase 1)
- batch size 8
- 30h of string recordings dataset

reproduce [[RAVE]] on graz_mini, snares
6 days on a Titan V, fuck
https://askgeek.io/en/gpus/vs/NVIDIA_Tesla-T4-vs-NVIDIA_TITAN-V
## misc
oh god the decomposition also expands the receptive field, it's 2good
## next
start with baselines so I know what I'm competing against
- run [[RAVE]] on `graz_mini` with a similar budget and see how it performs
- might need to modify the config a bit?
then, decomposition
then, unconditional generation
- [audio in ipython](https://stackoverflow.com/questions/54417598/playing-audio-in-jupyter-in-a-for-loop)
and normalizing flows