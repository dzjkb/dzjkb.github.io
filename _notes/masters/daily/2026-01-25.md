## experiments
ok same fkin shit what????
loss gets to around 8.7 and stops, why bruv
same thing whether I use logvar or scale parametrization
looks a bit like posterior collapse? the KL term falls lower than previous good runs, while reconstruction losses are ridiculously higher
**!** maybe it's something to do with capacity > latent size?
- ok maybe not
higher model capacity -> more precise learning at the start, indicated by the higher KL term
but somehow the signal is too noisy or weak and the decoder starts to learn on itself
while the KL term weight gets higher and the posterior starts collapsing
https://datascience.stackexchange.com/questions/48962/what-is-posterior-collapse-phenomenon
- `which in practice translates to producing some generic outputs x^ that are crude representatives of all seen x's.`
- yep that's exactly how the validation samples sound
- `difficult to make use of latent variables when coupled with a strong autoregressive decoder.` - the decoder is too strong
so, increase latent size?
and potentially decrease capacity again
~~no need to add the [free bits](https://stats.stackexchange.com/questions/267924/explanation-of-the-free-bits-technique-for-variational-autoencoders) thing for now probs~~
ok thx claude
```
Good question! Increasing latent dimensions could help, but it's a double-edged sword in this situation:

**Why it might help:**

- More dimensions = more chances for _some_ to learn useful representations
- If a few dimensions collapse, you still have others carrying information
- Can reduce pressure on individual dimensions to encode everything

**Why it might not help (or make things worse):**

- VAEs already tend to underutilize their latent capacity - you might just get more dead dimensions
- More dimensions = higher total KL for the same Î², which can make the collapse dynamics worse
- Doesn't address the underlying instability that's causing dimensions to fail in the first place

...

**Free bits**: This is really designed for this exact problem - it prevents dimensions from fully collapsing. Try 0.5-1.0 free bits per dimension.
```

running w/ free bits
- still collapsing are u for real
- logging dead dimensions
- added gradient clipping to 10.0
same shit still happening are you kidding me
interestingly enough gradient clipping doesn't influence the rate at which losses decrease
this is some bullshit man, is the decoder just too big at some point?
get more data w/e