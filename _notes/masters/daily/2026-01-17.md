## experiments
ok ok ok so
the loss jumps up with discriminator training and plateaus at around 4.2
but the quality improves heavily - the static is gone
although there's a different lower kind of static now in  each sound
not exactly a perfect reconstruction
ok maybe this is good enough for now
I mean it really does sound ok
next step would be to figure out where this plateau comes from, pretty much from the start
- the losses are slightly moving though
- feature matching loss + adversarial loss growing, discriminator loss falling
- reconstruction loss is plateauing
- so basically the discriminator is slowly learning, the decoder is staying the same
- => increase disciminator learning rate?
#### next
- why plateauing?
- reproduce [[RAVE]] on snares?
- model optimizations
- run without noise genereator

ok next run - no noise generator run, increased discriminator lr
and then - with more data? or maybe the noise strides should be larger/more filters?
#### unconditional gen
ok this is screwed up, worse than the last experiment
most of the generated sounds have artifacts like phasing or tremolo, weird tails
very noisy, not the static though
but the validation samples sound good though?? wtf
latent loss too high?
NFs coming in????
I guess this would be something visible in comparing the FAD/KID of the test set vs unconditional gen
experiment -> these metrics with/without NFs, goal is to show NFs give same good test reconstruction but better generation
## eval
ok run embedder + KID, compare with idk KID of the train+test sets
add KID to validation epoch
- when do I embed the validation set? need a standard location for that, like `<dir>_embeddings`
- the model doesn't have the dataset paths though, additional argument? yeah `kid_reference_set` makes sense
impl FAD, add to validation epoch
## datasets
fuck, go back to the drawing board