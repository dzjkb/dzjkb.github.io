## decomposition
`PQMF` continued
impl by [pytorch_sound](https://github.com/AppleHolic/pytorch_sound/blob/master/pytorch_sound/models/transforms.py), inspired by impl at [[Parallel WaveGan]]
- look pretty much identical

why is the [[RAVE]] impl longer?
study it -> add it
- how is it going to interact with the single latent vector (no time dimension)?
- does [[RAVE]] generate `N_BANDS` latent vectors?
`data_size = %N_BAND` (in `v2.gin`)
initial conv1d channel size in `EncoderV2` is `data_size * n_channels`
the returned channel size is `latent_size * n_out`
- `n_out: 2` in `v2.gin` - mean and variance
- ok nice
was added in [this commit](https://github.com/acids-ircam/RAVE/commit/2a93b2fcd451f847ad93f7368a84760b3c85ccc1#diff-2289bf435163bf9a35828e8c721deb1aff28ef44ec8f8d3e7350957708f1ba8b), just straight up all of it by Antoine

another impl from [hifi-gan](https://github.com/rishikksh20/multiband-hifigan/blob/master/pqmf.py)
- even shorter wtf
- oh but adapted from parallel wavegan oklol

[[RAVE]] only links to this shi https://ieeexplore.ieee.org/document/681427
- pdf https://files.core.ac.uk/download/pdf/216113507.pdf
- [[Parallel WaveGan]] links to the same thing

the cosine modulations are indeed the same between all implementations, but
- the `fmin` initial filter thingy?
- the whole `CachedPQMF` thing?

ok f it lesgo
don't do the Caching thing yet
- or should I? I keep deferring these things
- isn't this strictly connected to cached convolutions though?
wtf is this `x.ndim == 2` doing
it's always 3 anyway

did I ever reduce the latents to one vector????????????
nopeeeeeeeeeeeeeeeee
## later
single latent `z` - how do I decode this -> recover the low Hz signal?
- just repeat `z` a given amount of times?
- but how many times, given a single vec I have no info on how to reconstruct the length of the signal
- => has to be passed in as a parameter, fundamental change in model behaviour
    - but, theoretically, audio of any length could be passed in?
    - not if the latent reconstruction layer (fully connected?) has this length encoded in
- needs to be configurable

that phd vid watch
unconditional generation -> normalizing flows -> experimentsm/etrics
write to MaModrzejewski?
## MaModrzejewski review
experiments+evaluations+concept for review
[[Thesis overview]]
## training improvements
log less stuff
- no training
- validation way less often

https://docs.pytorch.org/docs/stable/fx.html#torch.fx.wrap