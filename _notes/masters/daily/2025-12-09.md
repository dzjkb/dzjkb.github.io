## experiments
ok ok
training/latent loss flattening out, validation loss totally flat - not increasing
that's a good sign
maybe I need more capacity _and_ more regularization?
or maybe I need to go adversarial from here

**next**
start phase 2 at step `97 * 2200 = 213400`
- do the calculations change if I change the batch size?
- the finished steps should be just saved
5 min per epoch oh god
1000 epochs would be 83h aaaaaa

or, another phase 1 training with a bit more capacity and more regularization
- what regularization?
- I don't really have any other than weightnorm which doesn't have any "intenisty" parameter
idk
maybe just more capacity? I feel like achieving better quality with PQMF should be trivially doable but it isn't...

gonna take a while until the loss drops back to sub-4 levels....
jeez
maybe I should try training a reasonable [[RAVE]] model on graz to know it's possible first
- notes from first reproduction run in [[2025-11-27]]
wait for first validation step and then stop it, try encoder training with bigger capacity
- ok ok the validation loss jumped considerably after 100 epochs of phase 2
- but the audio sounds completely different, less of the specific frequency static
- and more of a general reverb-y dark noise with a little bit of the piano coming out
- less harsh-digital, more natural
- fuckin >8h for 100 epochs I'm cooked
## later
that phd vid watch
unconditional generation (notebook ready, w8ing for reasonable results) -> normalizing flows -> experiments/metrics
write to MaModrzejewski? [[Thesis overview]]
## diversity
impl - take embeddings, use distance metrics in the embedding space
theory - ?
- [[DrumGAN]] ?
- KID as a way to measure "following of real data distribution" different than FAD - measuring the "plausibility"

**Kernel Inception Distance** - [Maximum Mean Discrepancy](https://arxiv.org/pdf/1901.03227) between last layers of an inception model
- drumgan trained a custom one on freesound drum samples
- on a "feature regression" task - brightness, hardness, depth, roughness, boominess, warmth, sharpness - all on a scale from 0 to 100
- oh god they're doing it on mel spectrograms so they can literally use an image inception model

maybe I could calculate KID on the same embeddings as FAD?
god the scope would be getting bigger and bigger if I were to train something custom
## embeddings
[CLAP](https://github.com/LAION-AI/CLAP)
[paper](https://arxiv.org/abs/2211.06687)
ready pypi package with `.get_audio_embedding_from_filelist()`
hell ye
trained on general audio/speech/environment/effects dataset, should be good
does well on classification audio tasks

ok got `mmd()` from [[DrumGAN]]
now, dl CLAP embeddings and run KID on two given sets

use `music_audioset_epoch_15_esc_90.14.pt`
[[Kernel Inception Distance]]
## seminar prez
what can I talk about
- some results would be nice
- normalizing flows
- DrumGAN, RAVE and generally the "creative" audio AI perspective (thx @mamodrzejewski)