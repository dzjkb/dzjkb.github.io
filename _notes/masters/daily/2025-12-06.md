## experiments
### graz
600 epochs, 500 encoder 100 adversarial
- encoder - ~5.5h
- adversarial - ~9.3h
all the losses just jump rapidly up in the adversarial phase except the discriminator loss
kind of makes sense if it's random weights at the beginning
wouldn't some sort of warmup for the feature matching loss weight make sense?
[[RAVE]] doesn't do that
- [ ] what's the slowdown for [[RAVE]] during adversarial training?

ok still static, fuck
the previous models seemed to learn something reasonable in this time, maybe the lr is too small?
- was constant 1e-4 previously and everything worked fine
but it only dropped to ~8e-5 (why?), shouldn't make that much of a difference

reached reconstruction losses:
- 2025-11-07 run: 3.1 in 50k steps
- 2025-12-05 run: 4.5 in 50k steps
something's up even before the adversarial phase
batch size 2 vs 4, so it was better quality at 2x less epochs ????
3h vs 13.3h though
hparams comparison

| run        | bsize | lr   | strides  | dilations | latent w |
| ---------- | ----- | ---- | -------- | --------- | -------- |
| 2025-11-07 | 2     | 1e-4 | 2 4 5 8  | 1 3 9     | 1e-4     |
| 2025-12-05 | 4     | 1e-4 | 2 3 5 10 | 1 3       | 1e-3     |
oh shit the latent loss maybe that's it?
stabilizes at 600 vs 60, makes sense

**next**
try the same but with less latent loss weight
- might result in worse latent space behaviour but more on that later I guess
I hope it's not a matter of the strides/dilations, but that might be worth checking too

bsize 40 maxes the card out, gives ~28s per epoch compared to ~40s earlier, nice
let's say it's 30s, 1000 epochs is going to be 8h
## misc
do something about the strides - length in samples dependency?