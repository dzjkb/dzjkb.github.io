## experiments
graz mini + snares mini + soft channel mini datasets

**graz mini**
126 training samples => 32 batches
300 epochs => 3200 * 3 steps => 9600 steps
doing 700 total, `graz_mini_base.yaml`
takes like 1min per epoch => 700 epochs => ~12h => 6-8.4$ => 20-30pln
ok that's still a bit much for the supposedly "cheap quick" training runs that I could use to fine-tune parameters

**soft channel mini**
ready, try later
## datasets
chopin scherzos
debussy preludes
## aws
do I just move development to AWS? doesn't make sense, need to have a way to just sync data to the instance once necessary
`rsync`?
```
rsync -avzhe "ssh -i ~/skrryt.pem" data/ awsgpu:/home/ubuntu/storydz/data/
```
nice

I can just run `poweroff` after a job to stop the instance, great
I could also add this to tailscale to not have to change the address after each start
## goals
ok fuck regroup what was the idea the plan
datasets datasets just finish the last 2 datasets
could get going with unconditional generation and then normalizing flows
- I should read more about why the latent loss is what it is
- am I sampling correctly?
then maybe decomposition
- references - what was the work that did it before?
and then the RAVE/MelGAN baselines fuuuuuuuck
- or maybe just the [[RAVE]] baseline would do.......
## unconditional generation
mkay lesgo
`forward()` should actually just do `x -> x_hat` instead of loss calculations huh
but for generating stuff we need to just use the decoder itself anyway