sabotage - it works
case studies?

techdog 7 start from 9
## normalizing flows
ok maybe I could go ahead with adding them?
or maybe try to find some more examples first
a [blog post](https://bjlkeng.io/posts/variational-autoencoders-with-inverse-autoregressive-flows/) about the VAE with IAF paper, maybe worth checking out
oh shit and some open [coursework](https://www.depthfirstlearning.com/2021/VI-with-NFs) huhuh

**what could go wrong though?**
optimizing the prior while using it to regularize the posterior - seems like an unstable optimization situation
the encoder could become very specific, like in the non-variational auto-encoder setting? with the prior trying to mimic that

shit, how do I even train this prior?
ok ok I guess you could think of this like the [[RAVE]] prior
learned on top of the approximate posterior outputs
so the NF prior would learn to mimic the approximate posterior, kind of going the other way but w/e?
that's what the VAE prior works kinda do

**which normalizing flows?**
both prior works use Real NVP
should be easily to substitute

either `MaskedAffineFlow` or `AffineCouplingBlock` from `normflows`, which one is it?
the examples used `MaskedAffineFlow`
- [x] learn exactly what and why `MaskedAffineFlow` does

ok what are the dimensions here though bro
and tf is the loss here bro
![[Pasted image 20251224142829.png]]
so expectation of the pre-flow `z` - expectation of the reconstruction - expectation of this flow thingy wtf
I guess the last term would be the jacobians
~~this doesn't seem right if we're trying to regularize `q0` to be like `N(0, 1)` ??~~
no we're regularizing `z0` to be from `q0` where `z0` was drawn from `q0`, what, ok sure

**training procedure**
either 1-stage or 2-stage
would make a lot of sense to start learning the prior once the encoder is frozen for training stability
[Learning Energy-based Variational Latent Prior for VAEs (2025)](https://arxiv.org/html/2510.00260) does 2-stage with standard isotropic gaussian regularization in the first phase
prior would be a separate module from encoder/decoder

ok but in the 2stage do I sample random points for each training sample and try to fit the prior?
this forces totally random points to match the posterior on specific samples, irrespective of how the training samples relate to one another
=> creating totally random latent space structure
ok my bad, this probably doesn't use sampling but density estimation
maximize the density of each training sample ok ok ok
uffff

what are the dimensions???
experiment design - what n of layers to, what latent size to preserve the size?

**ablation studies**
1. learn a model with/without prior NFs, adjusting the latent size to keep the overall _decoder + prior_ size constant
    - to verify whether the decoder doesn't in fact just learn the NF if necessary
2. also with/without posterior NFs? seems like more of a clear cut case though for the NFs though
3. comparing prior learned from the beginning vs second stage prior?
## thesis overview
hit up mamodrzejewski in maybe like 2-3 days, give him a break